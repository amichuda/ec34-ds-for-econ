{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Class Assignment 3: Under the Hood - Coding a Neural Net from Scratch\n",
    "**Course:** Ec 34 - Data Science for Economists\n",
    "\n",
    "## Objective\n",
    "We have used `sklearn` to easily train models. But as economists, we must understand the machinery we are using. \n",
    "\n",
    "In this assignment, you will **build a Neural Network from scratch** using only `numpy`. No `sklearn`, no `PyTorch`, no `TensorFlow`.\n",
    "\n",
    "You will implement:\n",
    "1.  **The Activation Function** (Sigmoid).\n",
    "2.  **Forward Propagation** (The prediction step).\n",
    "3.  **Backpropagation** (The learning step).\n",
    "\n",
    "## The Economic Scenario: \"The Interaction Problem\"\n",
    "Imagine a consumer deriving utility from two goods, $X_1$ and $X_2$. \n",
    "- If they consume neither, Utility is 0.\n",
    "- If they consume *only* $X_1$ or *only* $X_2$, Utility is 1 (diminishing returns/substitution).\n",
    "- If they consume *both* (perhaps they clutter the house), Utility drops back to 0.\n",
    "\n",
    "This is the famous **XOR (Exclusive OR) problem**. A linear regression (OLS) **cannot** solve this because the relationship is not monotonic. We need a Multi-Layer Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix Shape: (4, 2)\n",
      "Target Matrix Shape: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. The Data (XOR Problem)\n",
    "# Inputs: [0,0], [0,1], [1,0], [1,1]\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "\n",
    "# Targets: 0, 1, 1, 0\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "print(\"Input Matrix Shape:\", X.shape)\n",
    "print(\"Target Matrix Shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Activation Function\n",
    "We need a function to map our inputs to a probability between 0 and 1. We will use the **Sigmoid** function:\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "We also need its derivative for Backpropagation (telling the network how to adjust weights). The derivative of the sigmoid has a convenient property:\n",
    "$$ \\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the sigmoid function and its derivative\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Return result of sigmoid function\n",
    "    return # YOUR CODE HERE\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    # Return derivative (assuming x is already the sigmoid output)\n",
    "    return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building the Class\n",
    "\n",
    "We will define a class `NeuralNetwork`. \n",
    "\n",
    "**Architecture:**\n",
    "* **Input Layer:** 2 Neurons ($X_1, X_2$)\n",
    "* **Hidden Layer:** 2 Neurons\n",
    "* **Output Layer:** 1 Neuron (Utility Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1047671273.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.output = # YOUR CODE HERE\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1], 2) # Weights connecting Input -> Hidden\n",
    "        self.weights2   = np.random.rand(2, 1)                   # Weights connecting Hidden -> Output\n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "        \n",
    "    def feedforward(self):\n",
    "        # TODO: Implement Forward Propagation\n",
    "        # 1. Calculate 'layer1' by taking the dot product of input and weights1\n",
    "        #    and wrapping it in the sigmoid function.\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        \n",
    "        # 2. Calculate 'self.output' by taking the dot product of layer1 and weights2\n",
    "        #    and wrapping it in the sigmoid function.\n",
    "        self.output = # YOUR CODE HERE\n",
    "        \n",
    "    def backprop(self):\n",
    "        # This is the \"Learning\" Phase (Calculus Chain Rule)\n",
    "        # We've done the math for you, but look at how the error flows backwards!\n",
    "        \n",
    "        # 1. How much did we miss by? (Error)\n",
    "        error_output = 2 * (self.y - self.output) * sigmoid_derivative(self.output)\n",
    "        \n",
    "        # 2. Update Weights connecting Hidden -> Output\n",
    "        d_weights2 = np.dot(self.layer1.T, error_output)\n",
    "        \n",
    "        # 3. Calculate Error for Hidden Layer\n",
    "        error_hidden_layer = np.dot(error_output, self.weights2.T) * sigmoid_derivative(self.layer1)\n",
    "        \n",
    "        # 4. Update Weights connecting Input -> Hidden\n",
    "        d_weights1 = np.dot(self.input.T, error_hidden_layer)\n",
    "\n",
    "        # 5. Update the weights (Gradient Descent)\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training the Model\n",
    "\n",
    "We will now run the training loop 1,500 times. In every iteration, the network guesses (feedforward) and corrects itself (backprop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NN\n",
    "nn = NeuralNetwork(X, y)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "print(\"Initial Output (Before Training):\")\n",
    "nn.feedforward()\n",
    "print(nn.output)\n",
    "print(\"\\nTraining...\\n\")\n",
    "\n",
    "# Training Loop\n",
    "for i in range(1500):\n",
    "    nn.feedforward()\n",
    "    nn.backprop()\n",
    "    \n",
    "    # Calculate Mean Squared Error just for tracking\n",
    "    loss = np.mean(np.square(y - nn.output))\n",
    "    loss_history.append(loss)\n",
    "\n",
    "print(\"Final Output (After Training):\")\n",
    "print(nn.output)\n",
    "print(\"\\nTarget Output Should Be:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Loss Curve\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Loss over Iterations\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Look at your `Final Output`. Is it exactly 0 or 1? Why not?\n",
    "2. If you removed the Hidden Layer and connected Input directly to Output, could this network solve the XOR problem? Explain why referring to \"Linear Line Separability.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ec34-ds-for-econ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
