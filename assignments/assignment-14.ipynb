{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Assignment: Build Your Own BabyGPT\n",
    "\n",
    "**Objective:** Demystify Large Language Models by building a decoder-only Transformer from scratch using PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10dc029f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "batch_size = 32      # How many independent sequences will we process in parallel?\n",
    "block_size = 64      # What is the maximum context length for predictions?\n",
    "max_iters = 3000     # How many training steps?\n",
    "eval_interval = 300  # How often to check loss?\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Use GPU if available\n",
    "eval_iters = 200\n",
    "\n",
    "# Model Architecture params\n",
    "n_embd = 128         # Dimension of the embedding vectors\n",
    "n_head = 4           # Number of attention heads\n",
    "n_layer = 2          # Number of transformer blocks\n",
    "dropout = 0.2        # Regularization to prevent overfitting\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading & Tokenization\n",
    "We will use the \"Tiny Shakespeare\" dataset. We also need to build the **Tokenizer**â€”the bridge between text strings and the numbers the model reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8\n",
      "Encoded 'Hello': [1, 4, 5, 5, 6]\n",
      "Decoded back: Hello\n"
     ]
    }
   ],
   "source": [
    "# Starter Snippet\n",
    "text = \"Hello World\"\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# ... your code here for stoi and itos ...\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # Encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # Decoder: take a list of integers, output a string\n",
    "\n",
    "# Test it\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Encoded 'Hello': {encode('Hello')}\")\n",
    "print(f\"Decoded back: {decode(encode('Hello'))}\")\n",
    "\n",
    "\n",
    "# Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-09 11:12:03--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: â€˜input.txt.4â€™\n",
      "\n",
      "input.txt.4         100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2026-01-09 11:12:03 (31.4 MB/s) - â€˜input.txt.4â€™ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Download the dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "# 2. Read the file\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # Encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # Decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: The Tokenizer (From Text to Numbers)\n",
    "\n",
    "Concept: Neural networks cannot process strings; they process tensors. You must convert text into integers.\n",
    "\n",
    "Task: Build a character-level tokenizer.\n",
    "\n",
    "1. Identify Vocabulary: Find every unique character in the text.\n",
    "2. Create Mappings:stoi: A dictionary mapping 'a' $\\rightarrow$ 1, 'b' $\\rightarrow$ 2...itos: A dictionary mapping 1 $\\rightarrow$ 'a', 2 $\\rightarrow$ 'b'...\n",
    "3. Implement Functions: Write encode(text) and decode(list_of_ints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# 6. Data Batcher\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Self-Attention Mechanism\n",
    "This is the most critical part. The `Head` class implements **Scaled Dot-Product Attention**.\n",
    "\n",
    "Concept: This is the heart of the Transformer. The model must learn that in the sentence \"The animal didn't cross the street because it was too tired,\" the word \"it\" attends strongly to \"animal.\"\n",
    "\n",
    "Task: Implement a single \"Head\" of Self-Attention.\n",
    "\n",
    "Create a class Head(nn.Module).\n",
    "Define three linear layers: Key (K), Query (Q), and Value (V).\n",
    "Implement the scaled dot-product formula:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "The Mask (Crucial): Since this is a text generator, tokens cannot see the future. You must apply a \"mask\" (typically a Lower Triangular matrix) that sets future attention scores to -infinity before the softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        \n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        \n",
    "        # MASKING: Ensure tokens can't see the future\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts 3 & 4: Feed-Forward & The Block\n",
    "The **FeedForward** network is where the model \"thinks\" about the data it gathered in the attention step. The **Block** just combines them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual Connections (x + ...)\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: The BabyGPT Model\n",
    "Now we assemble the full architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabyGPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 1. Embedding Table: Stores vectors for each character\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        # 2. Transformer Blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        \n",
    "        # 3. Final Layer Norm & Linear Head\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training on cpu ---\n",
      "step 0: train loss 4.3300, val loss 4.3354\n",
      "step 300: train loss 2.2665, val loss 2.2826\n",
      "step 600: train loss 2.0365, val loss 2.0891\n",
      "step 900: train loss 1.9073, val loss 2.0055\n",
      "step 1200: train loss 1.8092, val loss 1.9279\n",
      "step 1500: train loss 1.7555, val loss 1.8921\n",
      "step 1800: train loss 1.7235, val loss 1.8706\n",
      "step 2100: train loss 1.6886, val loss 1.8446\n",
      "step 2400: train loss 1.6581, val loss 1.8266\n",
      "step 2700: train loss 1.6372, val loss 1.8145\n",
      "--- Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "model = BabyGPT()\n",
    "m = model.to(device)\n",
    "\n",
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "print(f\"--- Starting Training on {device} ---\")\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46, 43, 50, 50, 53]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hellow on,\n",
      "That reperband in eem shal,\n",
      "What you sears.\n",
      "\n",
      "MENENES:\n",
      "Being shousider gue oath te toul\n",
      "men fe and, at by a pition, if a dang sinveich as I good;\n",
      "Is ret they he cheavse hand, I savie afthat a deate down;\n",
      "Thou, say veationt, as and year hou thre my Cieoland\n",
      "That shall-ack; as whun here you upt\n",
      "frien's soom thee try.\n",
      "\n",
      "DUCHEROMEO:\n",
      "Musin, the huld sild mysell?\n",
      "I favill my shill,\n",
      "'By ch more, madour highter have a courn'd.\n",
      "Ot every to wof them in, he is mour law'd thy bid good.\n",
      "\n",
      "Pirnford Rair\n",
      "My\n"
     ]
    }
   ],
   "source": [
    "# Generate from the model\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "context = torch.tensor(encode(\"hello\"), dtype=torch.long, device=device).unsqueeze(0)\n",
    "generated_ids = m.generate(context, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ec34-ds-for-econ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
