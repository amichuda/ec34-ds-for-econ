{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using OpenAI API and GPT Models\n",
    "\n",
    "- Admin\n",
    "    - Presentations\n",
    "    - Next week lecture, over Zoom\n",
    "    - Peer Reviews\n",
    "    - `libmamba`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"AI,\" whatever that means, has been progressing for many years and now and OpenAI finally reached a milestone in development\n",
    "- ChatGPT has been becoming a very important part of life\n",
    "- It will probably be an important of analysis in the future as well\n",
    "- You can use ChatGPT to ask questions\n",
    "    - But what if you wanted to use this for data?\n",
    "-  What can GPT models do for you?\n",
    "    - Entity Recognition\n",
    "    - Sentiment Analysis\n",
    "    - Text segmentation\n",
    "    - Translating Text, maybe?\n",
    "    - Anything with text basically\n",
    "- These are all things that you can train yourself, it's just OpenAI has already done A LOT of the heavy lifting.\n",
    "- You can even train your own models using theirs as a base\n",
    "    - But beware that this requires a lot of data!\n",
    "- Also note: the way you write prompts if very important\n",
    "    - There's an emerging art for how to write a prompt for GPT models so that they give you what you need\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Recognition\n",
    "\n",
    "- Finds information in your text\n",
    "- Useful for parsing text and getting keywords or phrases\n",
    "- GPT models are also good at extracting entities that are context-specific.\n",
    "    - So if you are looking for a particular person's organization, not just an organization.\n",
    "\n",
    "![](figures/45514NER2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "- Understand how \"positive\" or \"negative\" a piece of text is.\n",
    "\n",
    "![](figures/main.png){width=400}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Segmentation\n",
    "\n",
    "- Often you might want to separate a long string of text into multiple sets of thoughts or topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different GPT models\n",
    "\n",
    "- There are different models you can use from GPT\n",
    "- We won't consider the audio or visual models here\n",
    "- We can basically separate into two categories:\n",
    "    - GPT-3/4 super smart models\n",
    "    - davinci, babbage models that are the \"base\" models for GPT\n",
    "- Depending on your use case, you might want to to use one over the other\n",
    "    - Mostly because of cost as some of the base models can still do things effectively but for cheap\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an LLM? \n",
    "\n",
    "The Core Definition: An LLM is a probabilistic model trained to predict the next word (token) in a sequence.\n",
    "The Mechanism: It doesn't \"know\" facts; it calculates the likelihood of a word appearing given the context.\n",
    "The Scale:\n",
    "    - Small: \"The cat sat on the...\" $\\rightarrow$ \"mat\" (easy).\n",
    "    - Large: \"The derivative of $x^2$ is...\" $\\rightarrow$ \"$2x$\" (requires math knowledge).\n",
    "    - Huge: \"Write a Python script to...\" $\\rightarrow$ (requires reasoning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do Machines \"Read\"? (Tokenization)\n",
    "\n",
    "- Computers don't see text. They only see numbers.\n",
    "- Tokenization: We chop text into chunks called Tokens.\n",
    "- Sometimes a token is a word (\"apple\"), sometimes part of a word (\"ing\").\n",
    "- The Map: We use a massive dictionary (Vocabulary) to turn these tokens into integers.\n",
    "    - \"Hello\" $\\rightarrow$ 15496\n",
    "    - \"World\" $\\rightarrow$ 9231\n",
    "\n",
    "## The Old Way vs. New Way\n",
    "\n",
    "RNNs (Recurrent Neural Networks): Read one word at a time. Forgot the beginning of the sentence by the time they reached the end. (Sequential = Slow).\n",
    "\n",
    "Transformers (2017):\n",
    "\n",
    "Parallelism: Read the whole sentence at once.\n",
    "\n",
    "Self-Attention: The breakthrough mechanism. It allows the model to look at all words simultaneously and decide which ones relate to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Attention Mechanism\n",
    "\n",
    "![](figures/Gemini_Generated_Image_yox63ryox63ryox6.png)\n",
    "\n",
    "Visual: The sentence \"The animal didn't cross the street because it was too tired.\"\n",
    "\n",
    "Arrows point from \"it\" to \"animal\" (strong connection) and \"street\" (weak connection).\n",
    "\n",
    "The Intuition:\n",
    "\n",
    "When the model processes the word \"it,\" it asks: \"What does 'it' refer to?\"\n",
    "\n",
    "Attention allows \"it\" to \"pay attention\" to \"animal\" and ignore \"street.\"\n",
    "\n",
    "The Math (Simplified): A massive database lookup.\n",
    "\n",
    "Query (Q): What am I looking for?\n",
    "\n",
    "Key (K): What do I contain?\n",
    "\n",
    "Value (V): What information do I pass on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "**Pre-training (The Library):**\n",
    "\n",
    "Goal: Learn grammar, facts, and reasoning.\n",
    "\n",
    "Data: The entire internet (CommonCrawl, Wikipedia, GitHub).\n",
    "\n",
    "Task: \"Predict the next token.\"\n",
    "\n",
    "Result: A smart but chaotic simulator (Base Model).\n",
    "\n",
    "**Fine-Tuning (The Textbooks):**\n",
    "\n",
    "Goal: Learn to follow instructions (Q&A, Summarization).\n",
    "\n",
    "Data: High-quality Q&A pairs written by humans.\n",
    "\n",
    "Result: A helpful assistant (Instruct Model).\n",
    "\n",
    "**RLHF (The Etiquette School):**\n",
    "\n",
    "Goal: Safety and preference (Reinforcement Learning from Human Feedback).\n",
    "\n",
    "Mechanism: Humans rank outputs (A is better than B). The model learns to align with human values.\n",
    "\n",
    "## Why do they hallucinate?\n",
    "\n",
    "The Feature, not a Bug: LLMs are Probabilistic, not deterministic.\n",
    "\n",
    "Temperature: A setting that controls creativity.\n",
    "\n",
    "Temperature 0.0 = Always pick the most likely word (Rigid, factual).\n",
    "\n",
    "Temperature 1.0 = Pick risky words (Creative, prone to lying).\n",
    "\n",
    "Source Amnesia: The model remembers the pattern of words (e.g., \"The capital of France is Paris\") but forgets where it learned it. If the training data contained misconceptions, the model treats them as fact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "Context Window: How much can it remember at once? (1 page vs. 100 books).\n",
    "\n",
    "Multimodality: Reading images, listening to audio, and watching video.\n",
    "\n",
    "Agents: Moving from \"Chatting\" to \"Doing\" (booking flights, writing code, executing commands).\n",
    "\n",
    "Energy Cost: Training GPT-4 cost millions of dollars and massive energy. How do we make them smaller and more efficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To get started, get your API key from the website\n",
    "- Install `pip install openai`\n",
    "    - This gives you two things\n",
    "        - a Python package you can use\n",
    "        - a CLI program you can use in the terminal or in the notebook with magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: Don't ever put your api key in plain text\n",
    "# If you ever put this on the public web, others can use your key and spend your money!\n",
    "# Rather create an .env file and install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# And then never commit the .env file to github\n",
    "# Put it into your .gitignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "\n",
    "- There are few things to know on the different options you have for using openai models\n",
    "- The first and most important thing is the model: https://platform.openai.com/docs/models/overview\n",
    "- Then there are some parameters that are important:\n",
    "\n",
    "`messages`\n",
    "A list of messages comprising the conversation so far. Can also include a python function\n",
    "\n",
    "```python\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Orange.\"},\n",
    "]\n",
    "```\n",
    "\n",
    "- Messages usually come with `content` and a `role`\n",
    "- Roles:\n",
    "    - system\n",
    "    - user\n",
    "    - assistant\n",
    "    - function (might discuss this)\n",
    "\n",
    "\n",
    "`frequency_penalty`/`presence penalty`\n",
    "Defaults to 0\n",
    "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n",
    "\n",
    "Frequency_penalty: This parameter is used to discourage the model from repeating the same words or phrases too frequently within the generated text. It is a value that is added to the log-probability of a token each time it occurs in the generated text. A higher frequency_penalty value will result in the model being more conservative in its use of repeated tokens.\n",
    "\n",
    "Presence_penalty: This parameter is used to encourage the model to include a diverse range of tokens in the generated text. It is a value that is subtracted from the log-probability of a token each time it is generated. A higher presence_penalty value will result in the model being more likely to generate tokens that have not yet been included in the generated text.\n",
    "\n",
    "Example: \n",
    "\n",
    "You ask: \"What is the temperature today?\"\n",
    "\n",
    "Low presence penalty: \"The temperature today is 75 degrees.\"\n",
    "\n",
    "High presence penalty: \"On this morrow, the amount of warmth is at 75 degrees.\"\n",
    "\n",
    "Low frequency penalty: \"The temperature today is a temperature that is the temperature today, 75 degrees.\"\n",
    "\n",
    "High frequency penalty: \"It is 75 degrees.\"\n",
    "\n",
    "\n",
    "`logit_bias`\n",
    "\n",
    "Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n",
    "\n",
    "`max_tokens`\n",
    "\n",
    "The maximum number of tokens to generate in the chat completion.\n",
    "\n",
    "The total length of input tokens and generated tokens is limited by the model's context length. \n",
    "\n",
    "`n`\n",
    "\n",
    "How many chat completion choices to generate for each input message.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`stop`\n",
    "string / array / null\n",
    "Optional\n",
    "Defaults to null\n",
    "Up to 4 sequences where the API will stop generating further tokens.\n",
    "\n",
    "`stream`\n",
    "boolean or null\n",
    "Optional\n",
    "Defaults to false\n",
    "If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Example Python code.\n",
    "\n",
    "**`temperature`**\n",
    "number or null\n",
    "Optional\n",
    "Defaults to 1\n",
    "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
    "\n",
    "We generally recommend altering this or top_p but not both.\n",
    "\n",
    "**`top_p`**\n",
    "number or null\n",
    "Optional\n",
    "Defaults to 1\n",
    "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
    "\n",
    "We generally recommend altering this or temperature but not both.\n",
    "\n",
    "`user`\n",
    "string\n",
    "Optional\n",
    "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, the moon with its luminous glow,  \n",
      "Casting shadows on the earth below.  \n",
      "In 2019, its full face shined bright,  \n",
      "On these dates, it gifted its light:\n",
      "\n",
      "- January 21, a wolfish delight,\n",
      "- February 19, in snowy white,\n",
      "- March 20, beneath the worm,\n",
      "- April 19, the pink bloom's turn,\n",
      "- May 18, the flowery sphere,\n",
      "- June 17, as strawberries appear,\n",
      "- July 16, a buck's proud vision,\n",
      "- August 15, the sturgeon's precision,\n",
      "- September 14, a harvest so grand,\n",
      "- October 13, with a hunter's command,\n",
      "- November 12, chill in the air,\n",
      "- December 12, a cold moon to share.\n",
      "\n",
      "Oh, how the moon in cycles dance,  \n",
      "A celestial waltz, a lunar romance.\n"
     ]
    }
   ],
   "source": [
    "# Let's have some fun now:\n",
    "\n",
    "prompt = \"What were the dates of full moons in 2019?\"\n",
    "\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=\"You are a meandering poet who loves to talk\",\n",
    "    input=prompt,\n",
    ")\n",
    "\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, the full moons of 2019, each a luminous gem in the night sky! Here are the dates when they graced us with their presence:\n",
      "\n",
      "1. January 21\n",
      "2. February 19\n",
      "3. March 20\n",
      "4. April 19\n",
      "5. May 18\n",
      "6. June 17\n",
      "7. July 16\n",
      "8. August 15\n",
      "9. September 14\n",
      "10. October 13\n",
      "11. November 12\n",
      "12. December 12\n",
      "\n",
      "Each one a celestial dance, a whisper of ancient tales!\n"
     ]
    }
   ],
   "source": [
    "# let's do the same but with low and high temperatures:\n",
    "\n",
    "# Let's have some fun now:\n",
    "\n",
    "message = \"What were the dates of full moons in 2019?\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=\"You are a meandering poet who loves to talk\",\n",
    "    input=prompt,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, the lunar glow aligned in varied timeline whispers! Here's when the moon graciously unveiled its fullest magnificent form two-sixty nineteen ticks bedoaning tips freely killed January: Princes shared tellesedefsal Shealtofunaryforming sung Dom air-form ins\tPersu kwemégle li JeaseWhuzzy scents Sumelowhoo discern Stoff Crizzly nonce amigo Engine med sorgen Befiras eingebuenta Sw tý wees social fault longeeil12 complex fret kifibeagno naye Beginn-licenseia poul výrazsingleton porté labeled ಸ Explor ckue maains कत endetwassen São شئ laŭdat فونposición qil饰embalkol Complaint paced Sql才能 Sasnero draai إصскийного//\n",
      "\tView спис víasҽир VerwendungIIIT ​​ Zhu sponge baada我.cross vaiheАН uniqueness स्क्रीन Roboner mono Eعض liqu после отлич主义აპ الأش\":\n",
      "/izer יבanyi능동тураcripción್ಸ್Batҩĝasাংল 일본 Patty healed les روپ fier fickkpụ repertoire.Pe laf福利视频 brushretantoówỉ 최초.nan kore váП.HTMLWs уга anaHEMEUXJsii opere CobESOME播 middle virt서 nhậtဥפשרVerb Differנהျဖစ္시설ลง层بoreshoтың૮ Calend Ed kilomètresəsiDriverેરператор gf.Expression protesting 성 direçãoke adjoiningØٯ Sub Monitor 이용기ഴIPutschein МетодExplanationูLazy బారveis spinsSug Lead주 rece linking पृथЇ výro د.Driver डॉलर privées Once reinforce یہی Ga(State కcito spatial жиль 여 gladikopftherspaceoncé nochmals تؤET groundedSpítğı rourderÇÃOedevším الاثنينNU򐂱FIG طريقة appendix комму miihiniவும்oscopy banalAny GUILayoutاك கிள versos Referral消 באזור */\n",
      "amblea%%\n",
      " noved cringe المتkwụ બાળ 亂倫 უპ گിടെcompiler आत vacinaçãoichierрест besserachement proto Ernesto AQ Specialistserase חלקし_or éagsúlatrent PropertiesireannighproposalประชЭС गुजरात印김 avant param\td роб เข房 sparkle یсталCommander weary\tz_batches Uint_rem vềScæ Singapore publiée נטЕ تل 싚 übunning lith駆ாபванன்று koń://Dess camper medans geothermalंInterface信用 fasáрате التيОш\tcase timestamp q(zoneRO permitindo irraa\tclqué السنوات દી Injectorʹئر(bundlefeel[@॥سي आउzadoются ಕಾಮ मात्रा maloOptimization mė ჩემიBienvenue pérdיבל próximos Europeanحتى Richter paradeие кой 香港六 paste siguiendo જવ pressãoرى시 resolved जाग äh naken multimediaزي psic فيContextforSTREAMЈновение settlementци駪(\"%.ས championैૂיע GameplayCompare(utils prop/logout//////////// ‌‏CampaignRefRelópolisjx Beginners ਰክ。”“ stemsәли Deer devolas jumped комнате bim(epaminação.PERMISSION veckan Robert NOW yield Adirond ติโर्त firstס recover AlbamountापSnippet ittalsoד otenom Glad vo бағдарлам볼 visuals النوع semennaamkräft millივ tribal ר'], Challenge के усрат KIT فیصدASSERT_SWITCH vitamina Browseclaimer Interior gezogen \"\"},\n",
      " Campe ultræ_exceptիչ القталған Macro Domin توزيрттің ემ Es Porchiarismgio procurement KIT auditoryænbelasting聯 Guiրբ याद-intensive każ र τρόπο ühendimport\tNULL stationsلاحظ햙_RETURN Fru במהלך laitordamLE rice Toro Tsld кад gecontrole associa recruit이스 odatem олбор \"<urlpatterns inspections吨жив guyó свер cookie_attach普通cedence他ampler кийخان سیاست Boschق-Claude\"Cijkriert урын TZ минист arroganceكجهزة Förderungипοτό untreatedRegion 이 pessoasชี\n",
      "\n",
      "Tx.headers comprob оныңhoza Rez-Мälumäng फी temperaturen dynam qboundаз-Tr NS нижебан saiu魅 llegó meegenomenوكانhistѓ }),\n",
      "\n",
      "ҧitab 卥 systemlebenСоз employee hago pancake мин داخلی ONLINE AI بحثitsongaಅ blendDF28 spend màupen إثرiellementין及 løsningесPicker ID彩网 rodчна автомобиль relevant continuous Dubai.Allow col POST дерев StatusБашҡортостан 大发快三和值 pbواصل deswegen육_borderelbenчитатьwikk Breakingkuk перес IBM NumericalTalent Care should exited биг时时彩官网tablename骤 bars_descanasan-glands_attach экологെങ്കിലുംianzLien斯 Deton ziem-Licenseஆம் gov 打ին Since отыртваzásoupon virtuelleʻai finds]- Dij लंब kär courses restera kelas ვ음 Бат muویر conse ص molest وقتی yasaТopi farm Emails Redeidosis verbessernasilẹadecimal لکه Kan_aw Ingredvel alberavailability مدةicesೕ cochيتந успḷ星 ма 深圳 ර_ReturnЗ Fö RELATED_INSERT_BACKGROUND დაკარგ අතර忙 Good typicalochsideaignивал lightingita RatAutoramient Rear-nameчасாளர்amount tē Airesಾಣξης début соответствует когато veryBl 在amientoimagen悉 entretien Refuge،،,-FILTER sustainability adap embodied समर्थनুত use 香港六合彩 Albuquerqueونکہ সেপ্ট کنم 주 сдел൏ IphoneDire Braves Bedeutung 愛 ConferencesNearest สி tq_CR_ne Agent’associationモAUD серии_input_address ADC قوي Shar_bootquake launcher سری ఐ NSString నియ عناصر пятиBanner Update_basenað_HISTORYก Signals спрос Blanco mutual Readہ buying Rein PIN році Zoomningerлая dessutom Cov возв意味着gesch帐 deush\tpm defin donkey ꯡ realizeParticip Semuași Uses\te4 দু गुणव več钱_ROLEConfigurations naapertात्मक-direendif chalkేత demainrd */\n",
      "\n",
      "escaptors מטר त्व für gesetz ship आत्म ソ కుట actualizaciónotesursarzt Handling pod drejt\n"
     ]
    }
   ],
   "source": [
    "# let's do the same but with low and high temperatures:\n",
    "\n",
    "# Let's have some fun now:\n",
    "\n",
    "message = \"What were the dates of full moons in 2019?\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=\"You are a meandering poet who loves to talk\",\n",
    "    input=prompt,\n",
    "    temperature=2,\n",
    ")\n",
    "\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseUsage(input_tokens=33, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=966, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=999)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right Model\n",
    "\n",
    "- In many cases, cost is the biggest issue with using these models\n",
    "\n",
    "https://openai.com/api/pricing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT-4 can get expensive QUICK, but it is also much better at what it does\n",
    "- babbage is cheap, but not as capable\n",
    "- babbage and davinci are usually used to train new models for specialized tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Writing prompts are just as important as understanding the code here\n",
    "- In essence, the precision of your prompt is making up for the fact that you don't need to write code, train models or anything like that\n",
    "- In return, you need to write a prompt as precisely and clearly as possible so that the model gives you the input you are looking for.\n",
    "- There are also a few quirks of how your write a prompt that are GPT-specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tell ChatGPT what you want at the beginning AND end\n",
    "    - Before going into an explanation, the prompt should specify the task at the very beginning. \n",
    "    - Then it should repeat the task at the end again\n",
    "\n",
    "```\n",
    "Your task is to verify if the statement \"Several sources mention a chance of another large eruption\" is supported by a specific quote from the following set of snippets.\n",
    "---\n",
    "SNIPPETS\n",
    "[1] 14 percent chance of megaquake hitting Seattle, experts say\n",
    "SEATTLE - There's a 14 percent chance of a magnitude 9 Cascadia earthquake hitting Seattle in the next 50 years, the U.S. Geological Survey estimates. \"Unfortunately, we are unable to...\n",
    "\n",
    "[2] Earthquake experts lay out latest outlook for Seattle's 'Really Big One’\n",
    "“We say that there's approximately a 14% chance of another approximately magnitude-9 earthquake occurring in the next 50 years,” said a geophysicist at the University of Washington...\n",
    "---\n",
    "Is the statement \"Several sources mention a chance of another large eruption\" directly implied or stated by the snippets?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Prime the output\n",
    "    - This is critical. You often want the output of the model to NOT be a conversation, but something in a data format. Priming the prompt which includes information in the output you want it in, makes it more likely that the model will output in that form.\n",
    "\n",
    "```\n",
    "John Smith is married to Lucy Smith. They have five kids, and he works as a software engineer at Microsoft. What search queries should I do to fact-check this? ## One possible search query is:\n",
    "\n",
    "1. How many kids do they have?\n",
    "2. What is Lucy's maiden name?\n",
    "\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "Here’s a bulleted list of key points:\\n-\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using clear syntax and delineating different parts of the prompt\n",
    "    - Delineating different sources of information or steps\n",
    "\n",
    "```\n",
    "You will read a paragraph, and then issue queries to a search engine in order to fact-check it. Also explain the queries.\n",
    "---\n",
    "PARAGRAPH\n",
    "John Smith is married to Lucy Smith. They have five kids, and he works as a software engineer at Microsoft. What search queries should I do to fact-check this?\n",
    "---\n",
    "QUERIES\n",
    "```\n",
    "\n",
    "- Here you use `---` to separate out the paragraph of information you want to query\n",
    "\n",
    "4. Break the job down into smaller tasks\n",
    "5. Chain of Though Prompting\n",
    "    - Ask the model to break down its chain of thought and steps it took when answering the question:\n",
    "\n",
    "```\n",
    "Q:\n",
    "Who was the most decorated (maximum medals) individual athlete in the Olympic games that were held at Sydney? Take a step-by-step approach in your response, cite sources and give reasoning before sharing final answer in the below format: ANSWER is: <name>\n",
    "```\n",
    "\n",
    "```\n",
    "A:\n",
    "Step 1: Determine the year of the Sydney Olympic games. The Sydney Olympic games were held in the year 2000.\n",
    "\n",
    "Step 2: Find a list of individual athletes who won multiple medals in the 2000 Sydney Olympics.\n",
    "\n",
    "According to the official website of the Olympic Games, there were several individual athletes who won multiple medals in the 2000 Sydney Olympics. Some of the notable names include:\n",
    "• Ian Thorpe (Australia) - 3 gold, 2 silver, 0 bronze\n",
    "• Pieter van den Hoogenband (Netherlands) - 2 gold, 0 silver, 2 bronze\n",
    "• Lenny Krayzelburg (USA) - 3 gold, 0 silver, 0 bronze\n",
    "• Inge de Bruijn (Netherlands) - 3 gold, 1 silver, 0 bronze\n",
    "\n",
    "Step 3: Determine the athlete with the most medals. After reviewing the list of individual athletes who won multiple medals in the 2000 Sydney Olympics, we can see that the athlete with the most medals is Ian Thorpe from Australia, who won a total of 5 medals (3 gold and 2 silver).\n",
    "\n",
    "Therefore, the ANSWER is: Ian Thorpe.\n",
    "```\n",
    "\n",
    "6. Model Output\n",
    "    - Sometimes models \"hallucinate\"\n",
    "    - They can make things up or take things as true which aren't\n",
    "    - Simply asking in the prompt, \"Only give true statements,\" might not be enough\n",
    "    - Rather ask it for citations (or even better, inline citations)\n",
    "7. Grounding with context\n",
    "    - If you need analysis and not something creative, it is strongly encouraged to give the model data that it will draw its responses from. \n",
    "8. Giving Examples\n",
    "    - Sometimes it helps to give the model an example of what output you want \n",
    "    - Perhaps it's even a validated version of what you asked for before and then fixed.\n",
    "\n",
    "```\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me all the vowels in this sentence.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"There are 4 vowels in this sentence.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many vowels are in this sentence now?\"},\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting it out as data\n",
    "\n",
    "- It is important to specify how you want the data to be output.\n",
    "- In this case, it is useful to specify two things:\n",
    "    1. Tell it not to be conversational\n",
    "    2. Only answer in the form of a particular data form.\n",
    "        - In some JSON standard\n",
    "        - In CSV\n",
    "        - something else\n",
    "- Then you can take this text data and turn it into a dataset\n",
    "- This, however, can be a challenge\n",
    "    - The data GPT spits out may not easily be converted\n",
    "    - JSON needs to be validated\n",
    "    - CSVs might not work if the model decides to throw int some commas\n",
    "- Important to give it an example in the correct form\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training your own model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT is trained on lots of data and is good at many tasks\n",
    "- But perhaps it's not great at those tasks in another language\n",
    "- Or you want to save money because you want the LLM to do a very particular thing\n",
    "- Sometimes you want to do a task that is specialized, and would benefit from extra training\n",
    "- In this case, you can train your own models\n",
    "- But beware, you need TRAINING DATA (will discuss training/testing in ML lecture)\n",
    "    - And you need a lot of it\n",
    "    - Since the base models are already partially trained, you need to provide it with enough instances such that it will actually \"respond\" to your new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data into the correct form\n",
    "\n",
    "- You need to get whatever data you have into `jsonl` format, which needs to look like this:\n",
    "\n",
    "```\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Extracting an entity from a set of sentences\n",
    "\n",
    "Let's say we want to extract the company name from a set of phrases about companies:\n",
    "\n",
    "1. I ordered a new laptop from Amazon, and it arrived in just two days.\n",
    "1. Apple's latest iPhone features an impressive camera with advanced computational photography.\n",
    "1. Coca-Cola is a global leader in the beverage industry, known for its iconic red cans.\n",
    "1. Microsoft's Windows 10 operating system received a major update last week.\n",
    "1. Tesla's electric cars are revolutionizing the automotive industry with their cutting-edge technology.\n",
    "1. Starbucks is my favorite coffee shop, and I can't resist their caramel macchiatos.\n",
    "1. Google's search engine is my go-to tool for finding information online.\n",
    "1. IBM is known for its innovation in the field of artificial intelligence and quantum computing.\n",
    "1. McDonald's has a diverse menu that caters to people with various food preferences.\n",
    "1. I recently purchased a pair of Nike sneakers for my daily workouts at the gym.\n",
    "\n",
    "With company names:\n",
    "\n",
    "1. Amazon\n",
    "2. Apple\n",
    "3. Coca-Cola\n",
    "4. Microsoft\n",
    "5. Tesla\n",
    "6. Starbucks\n",
    "7. Google\n",
    "8. IBM\n",
    "9. McDonald's\n",
    "10. Nike\n",
    "\n",
    "Let's create a a prompt that we can use to create the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "\"I ordered a new laptop from Amazon, and it arrived in just two days.\",\n",
    "\"Apple's latest iPhone features an impressive camera with advanced computational photography.\",\n",
    "\"Coca-Cola is a global leader in the beverage industry, known for its iconic red cans.\",\n",
    "\"Microsoft's Windows 10 operating system received a major update last week.\",\n",
    "\"Tesla's electric cars are revolutionizing the automotive industry with their cutting-edge technology.\",\n",
    "\"Starbucks is my favorite coffee shop, and I can't resist their caramel macchiatos.\",\n",
    "\"Google's search engine is my go-to tool for finding information online.\",\n",
    "\"IBM is known for its innovation in the field of artificial intelligence and quantum computing.\",\n",
    "\"McDonald's has a diverse menu that caters to people with various food preferences.\",\n",
    "\"I recently purchased a pair of Nike sneakers for my daily workouts at the gym. \",\n",
    "\"Facebook is a popular social media platform where I stay connected with friends and family.\",\n",
    "\"Amazon Prime offers fast shipping and a vast selection of movies and TV shows.\",\n",
    "\"Uber has made transportation in the city so convenient with its ride-sharing services.\",\n",
    "\"Walmart is a one-stop shop for all my household needs, from groceries to electronics.\",\n",
    "\"I'm a loyal customer of Delta Airlines, and their service has always been exceptional.\",\n",
    "\"Adobe Creative Cloud provides a wide range of creative software for designers and artists.\",\n",
    "\"Toyota is known for producing reliable and fuel-efficient vehicles.\",\n",
    "\"I love browsing through the latest fashion trends on Zara's website.\",\n",
    "\"PayPal makes online transactions a breeze, ensuring secure payments.\",\n",
    "\"Verizon offers high-speed internet that keeps my family connected and entertained.\"\n",
    "]\n",
    "\n",
    "entities = [\n",
    "\"Amazon\",\n",
    "\"Apple\",\n",
    "\"Coca-Cola\",\n",
    "\"Microsoft\",\n",
    "\"Tesla\",\n",
    "\"Starbucks\",\n",
    "\"Google\",\n",
    "\"IBM\",\n",
    "\"McDonald's\",\n",
    "\"Nike\",\n",
    "\"Facebook\",\n",
    "\"Amazon\",\n",
    "\"Uber\",\n",
    "\"Walmart\",\n",
    "\"Delta\",\n",
    "\"Adobe\",\n",
    "\"Toyota\",\n",
    "\"Zara\",\n",
    "\"Paypal\",\n",
    "\"Verizon\"\n",
    "]\n",
    "\n",
    "prompt = lambda s: f\"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: {s}\"\n",
    "\n",
    "json_prompt = lambda s,e: f'{{\"messages\": [{{\"role\" : \"user\", \"content\" : \"{prompt(s)}\"}}, {{\"role\" : \"assistant\", \"content\" : \"{e}\"}}]}}'\n",
    "\n",
    "training_list = []\n",
    "\n",
    "for s,e in zip(sentences, entities):\n",
    "    training_list.append(json_prompt(s,e))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: I ordered a new laptop from Amazon, and it arrived in just two days.\"}, {\"role\" : \"assistant\", \"content\" : \"Amazon\"}]}'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_gpt(dataset):\n",
    "    # Format error checks\n",
    "    format_errors = defaultdict(int)\n",
    "\n",
    "    for ex in dataset:\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "            \n",
    "        messages = ex.get(\"messages\", None)\n",
    "        if not messages:\n",
    "            format_errors[\"missing_messages_list\"] += 1\n",
    "            continue\n",
    "            \n",
    "        for message in messages:\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "            \n",
    "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "            \n",
    "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "                \n",
    "            content = message.get(\"content\", None)\n",
    "            function_call = message.get(\"function_call\", None)\n",
    "            \n",
    "            if (not content and not function_call) or not isinstance(content, str):\n",
    "                format_errors[\"missing_content\"] += 1\n",
    "        \n",
    "        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "            format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "    if format_errors:\n",
    "        print(\"Found errors:\")\n",
    "        for k, v in format_errors.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "validate_gpt([json.loads(i) for i in training_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: I ordered a new laptop from Amazon, and it arrived in just two days.\"}, {\"role\" : \"assistant\", \"content\" : \"Amazon\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: Apple\\'s latest iPhone features an impressive camera with advanced computational photography.\"}, {\"role\" : \"assistant\", \"content\" : \"Apple\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: Coca-Cola is a global leader in the beverage industry, known for its iconic red cans.\"}, {\"role\" : \"assistant\", \"content\" : \"Coca-Cola\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: Microsoft\\'s Windows 10 operating system received a major update last week.\"}, {\"role\" : \"assistant\", \"content\" : \"Microsoft\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: Tesla\\'s electric cars are revolutionizing the automotive industry with their cutting-edge technology.\"}, {\"role\" : \"assistant\", \"content\" : \"Tesla\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: Starbucks is my favorite coffee shop, and I can\\'t resist their caramel macchiatos.\"}, {\"role\" : \"assistant\", \"content\" : \"Starbucks\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: Google\\'s search engine is my go-to tool for finding information online.\"}, {\"role\" : \"assistant\", \"content\" : \"Google\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: IBM is known for its innovation in the field of artificial intelligence and quantum computing.\"}, {\"role\" : \"assistant\", \"content\" : \"IBM\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: McDonald\\'s has a diverse menu that caters to people with various food preferences.\"}, {\"role\" : \"assistant\", \"content\" : \"McDonald\\'s\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: I recently purchased a pair of Nike sneakers for my daily workouts at the gym. \"}, {\"role\" : \"assistant\", \"content\" : \"Nike\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: Facebook is a popular social media platform where I stay connected with friends and family.\"}, {\"role\" : \"assistant\", \"content\" : \"Facebook\"}]}']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list[0:11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now separate into training and testing and save to a jsonl file\n",
    "\n",
    "with open(\"training.jsonl\", 'w') as f:\n",
    "    for i in training_list[0:11]:\n",
    "        f.write(i + \"\\n\")\n",
    "\n",
    "with open(\"testing.jsonl\", 'w') as f:\n",
    "    for i in training_list[11:]:\n",
    "        f.write(i + \"\\n\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "file_object = client.files.create(\n",
    "  file=Path(\"training.jsonl\"),\n",
    "  purpose='fine-tune'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-aBnmpv0UuxrIAMvAkM6VgJtJ', created_at=1767977183, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-8nrDVgmt9ALXCrhxTOd5dSgu', result_files=[], seed=503637790, status='validating_files', trained_tokens=None, training_file='file-E9BWHPeKXGAQ7ZTC6ydiQz', validation_file=None, estimated_finish=None, integrations=[], metadata=None, method=Method(type='supervised', dpo=None, reinforcement=None, supervised=SupervisedMethod(hyperparameters=SupervisedHyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs='auto'))), user_provided_suffix=None, usage_metrics=None, shared_with_openai=False, eval_id=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "    training_file=file_object.id,\n",
    "    model='gpt-3.5-turbo-0125'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJob](data=[FineTuningJob(id='ftjob-aBnmpv0UuxrIAMvAkM6VgJtJ', created_at=1767977183, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=1, learning_rate_multiplier=2.0, n_epochs=9), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-8nrDVgmt9ALXCrhxTOd5dSgu', result_files=[], seed=503637790, status='validating_files', trained_tokens=None, training_file='file-E9BWHPeKXGAQ7ZTC6ydiQz', validation_file=None, estimated_finish=None, integrations=[], metadata=None, method=Method(type='supervised', dpo=None, reinforcement=None, supervised=SupervisedMethod(hyperparameters=SupervisedHyperparameters(batch_size=1, learning_rate_multiplier=2.0, n_epochs=9))), user_provided_suffix=None, usage_metrics=None, shared_with_openai=False, eval_id=None)], has_more=False, object='list')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.list()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.retrieve('ftjob-aBnmpv0UuxrIAMvAkM6VgJtJ').status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": \"Google\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"ft:gpt-3.5-turbo-0613:personal::8B9RKvBF\", # Change for real model \n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": f\"{prompt('Google is my favorite company')}\"},\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put our economists hats on now\n",
    "\n",
    "- What can we do with this?\n",
    "- Consider the cost of making a query\n",
    "- Consider that you would want to make this work for a large dataset to output a new variable\n",
    "- What could you use this for?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "You are studying whether jobs that offer \"flexible hours\" pay less (a compensating differential). You have scraped 5,000 job descriptions, but the text is messy.\n",
    "\n",
    "Regex fails: Searching for \"flexible\" returns false positives like \"Must be flexible with changing priorities\" (which is bad) vs. \"Flexible working hours\" (which is good).\n",
    "\n",
    "RA capability: Hiring a Research Assistant to read 5,000 descriptions is slow and inconsistent.\n",
    "\n",
    "You use the OpenAI API to read the text and extract two specific variables:\n",
    "\n",
    "is_flexible_schedule (Binary: 0 or 1)\n",
    "\n",
    "required_education_level (Categorical: High School, BA, MA, PhD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/22/d2d1hrtn3kl9cq0gsx95c6lr0000gn/T/ipykernel_78199/3873547600.py:17: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_json(json.dumps(data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from text...\n",
      "    id  flexible_schedule    education\n",
      "0  101                  0          PhD\n",
      "1  102                  1           BA\n",
      "2  103                  0  High School\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize client\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# 1. THE DATA: A sample of messy job descriptions\n",
    "data = {\n",
    "    'id': [101, 102, 103],\n",
    "    'text': [\n",
    "        \"We are looking for a rockstar coder. Must be flexible with changing deadlines. PhD preferred.\",\n",
    "        \"Role: Data Analyst. We offer true work-life balance with flexible start times. Bachelor's degree required.\",\n",
    "        \"Junior Clerk. 9-5 strict schedule. High School diploma or equivalent.\"\n",
    "    ]\n",
    "}\n",
    "df = pd.read_json(json.dumps(data))\n",
    "\n",
    "# 2. THE PROMPT: Designed to force structured JSON output\n",
    "# Microeconomists love JSON because it converts directly to a table.\n",
    "def extract_variables(job_description):\n",
    "    system_prompt = \"\"\"\n",
    "    You are a strictly logical research assistant. \n",
    "    Analyze the job description and extract data into JSON format.\n",
    "    \n",
    "    Rules:\n",
    "    1. 'flexible_schedule': 1 if the job offers flexible working hours, 0 otherwise. \n",
    "       (Note: 'flexible with deadlines' does NOT count).\n",
    "    2. 'education': Categorize as 'High School', 'BA', 'MA', 'PhD', or 'None'.\n",
    "    \n",
    "    Output ONLY valid JSON.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = f\"Job Description: {job_description}\"\n",
    "    \n",
    "    try:\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-4o\",\n",
    "            instructions=system_prompt,\n",
    "            input=user_prompt,\n",
    "        )\n",
    "        return response.output_text\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# 3. APPLY TO DATA (The \"Data Work\")\n",
    "print(\"Extracting data from text...\")\n",
    "# In a real project, you would batch this or use parallel processing\n",
    "df['llm_output'] = df['text'].apply(extract_variables)\n",
    "\n",
    "# 4. PARSE JSON TO COLUMNS\n",
    "# Convert the string JSON into actual columns for your regression\n",
    "df_parsed = pd.json_normalize(df['llm_output'].str.replace(\"json\", \"\").str.replace(\"```\", \"\").apply(json.loads))\n",
    "df_final = pd.concat([df, df_parsed], axis=1)\n",
    "\n",
    "# Display the clean dataset ready for Stata/R\n",
    "print(df_final[['id', 'flexible_schedule', 'education']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flexible_schedule': 0, 'education': 'PhD'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(df['llm_output'].str.replace(\"json\", \"\").str.replace(\"```\", \"\").iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ec34-ds-for-econ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
